# Concurrency and Asynchronous Programming in Python

## Table of Contents
1. [Threading](#threading)
2. [Multiprocessing](#multiprocessing)
3. [Async/Await](#async-await)
4. [Coroutines](#coroutines)
5. [Event Loops](#event-loops)
6. [Concurrent Execution](#concurrent-execution)
7. [Synchronization](#synchronization)
8. [Best Practices](#best-practices)

## Threading

### Basic Threading

```python
import threading
from typing import List, Callable, Any
import time
import queue

class ThreadWorker:
    """Handle thread-based operations."""
    
    def __init__(self, num_threads: int = 4) -> None:
        self.num_threads = num_threads
        self.task_queue: queue.Queue = queue.Queue()
        self.result_queue: queue.Queue = queue.Queue()
        self.threads: List[threading.Thread] = []
    
    def worker(self) -> None:
        """Worker function for processing tasks."""
        while True:
            try:
                task = self.task_queue.get(timeout=1)
                if task is None:
                    break
                
                func, args = task
                try:
                    result = func(*args)
                    self.result_queue.put((True, result))
                except Exception as e:
                    self.result_queue.put((False, str(e)))
                
                self.task_queue.task_done()
            except queue.Empty:
                continue
    
    def start(self) -> None:
        """Start worker threads."""
        self.threads = []
        for _ in range(self.num_threads):
            thread = threading.Thread(target=self.worker)
            thread.daemon = True
            thread.start()
            self.threads.append(thread)
    
    def stop(self) -> None:
        """Stop worker threads."""
        for _ in range(self.num_threads):
            self.task_queue.put(None)
        for thread in self.threads:
            thread.join()
    
    def add_task(self, func: Callable, *args: Any) -> None:
        """
        Add task to queue.
        
        Args:
            func: Function to execute
            *args: Function arguments
        """
        self.task_queue.put((func, args))
    
    def get_results(self) -> List[Any]:
        """
        Get task results.
        
        Returns:
            List[Any]: List of results
        """
        results = []
        while not self.result_queue.empty():
            success, result = self.result_queue.get()
            if success:
                results.append(result)
            else:
                print(f"Task failed: {result}")
        return results
```

### Thread Pool

```python
from concurrent.futures import ThreadPoolExecutor
from typing import List, Callable, Any
import time

class ThreadPool:
    """Manage thread pool operations."""
    
    def __init__(self, max_workers: int = None) -> None:
        self.max_workers = max_workers
    
    def map(
        self,
        func: Callable,
        items: List[Any],
        timeout: Optional[float] = None
    ) -> List[Any]:
        """
        Map function over items using thread pool.
        
        Args:
            func: Function to execute
            items: Items to process
            timeout: Execution timeout
            
        Returns:
            List[Any]: Results
        """
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            try:
                results = list(
                    executor.map(func, items, timeout=timeout)
                )
                return results
            except TimeoutError:
                print("Execution timed out")
                return []
    
    def submit_tasks(
        self,
        tasks: List[Tuple[Callable, tuple]],
        timeout: Optional[float] = None
    ) -> List[Any]:
        """
        Submit multiple tasks to thread pool.
        
        Args:
            tasks: List of (function, args) tuples
            timeout: Execution timeout
            
        Returns:
            List[Any]: Results
        """
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(func, *args)
                for func, args in tasks
            ]
            
            results = []
            for future in futures:
                try:
                    result = future.result(timeout=timeout)
                    results.append(result)
                except TimeoutError:
                    print("Task timed out")
                except Exception as e:
                    print(f"Task failed: {e}")
            
            return results
```

## Multiprocessing

### Process Pool

```python
from multiprocessing import Pool, cpu_count
from typing import List, Callable, Any
import time

class ProcessPool:
    """Manage process pool operations."""
    
    def __init__(self, processes: Optional[int] = None) -> None:
        self.processes = processes or cpu_count()
    
    def map(
        self,
        func: Callable,
        items: List[Any],
        chunks: Optional[int] = None
    ) -> List[Any]:
        """
        Map function over items using process pool.
        
        Args:
            func: Function to execute
            items: Items to process
            chunks: Chunk size for processing
            
        Returns:
            List[Any]: Results
        """
        with Pool(processes=self.processes) as pool:
            if chunks:
                results = pool.map_async(
                    func,
                    items,
                    chunksize=chunks
                ).get()
            else:
                results = pool.map(func, items)
            return results
    
    def imap(
        self,
        func: Callable,
        items: List[Any],
        chunks: Optional[int] = None
    ) -> Iterator[Any]:
        """
        Iterator version of map.
        
        Args:
            func: Function to execute
            items: Items to process
            chunks: Chunk size for processing
            
        Returns:
            Iterator[Any]: Results iterator
        """
        with Pool(processes=self.processes) as pool:
            if chunks:
                yield from pool.imap(
                    func,
                    items,
                    chunksize=chunks
                )
            else:
                yield from pool.imap(func, items)
```

### Shared Memory

```python
from multiprocessing import Process, Value, Array
from typing import List
import ctypes

class SharedMemoryManager:
    """Manage shared memory between processes."""
    
    @staticmethod
    def create_counter() -> Value:
        """
        Create shared counter.
        
        Returns:
            Value: Shared counter
        """
        return Value(ctypes.c_int, 0)
    
    @staticmethod
    def create_array(size: int) -> Array:
        """
        Create shared array.
        
        Args:
            size: Array size
            
        Returns:
            Array: Shared array
        """
        return Array(ctypes.c_int, size)
    
    @staticmethod
    def increment_counter(counter: Value, amount: int = 1) -> None:
        """
        Increment shared counter.
        
        Args:
            counter: Shared counter
            amount: Amount to increment
        """
        with counter.get_lock():
            counter.value += amount
    
    @staticmethod
    def update_array(array: Array, index: int, value: int) -> None:
        """
        Update shared array.
        
        Args:
            array: Shared array
            index: Index to update
            value: New value
        """
        with array.get_lock():
            array[index] = value
```

## Async/Await

### Async Functions

```python
import asyncio
from typing import List, Any, Optional
import aiohttp
import time

class AsyncOperations:
    """Handle asynchronous operations."""
    
    @staticmethod
    async def fetch_url(
        url: str,
        timeout: Optional[float] = None
    ) -> str:
        """
        Fetch URL content asynchronously.
        
        Args:
            url: URL to fetch
            timeout: Request timeout
            
        Returns:
            str: Response content
        """
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=timeout) as response:
                return await response.text()
    
    @staticmethod
    async def fetch_multiple_urls(
        urls: List[str],
        timeout: Optional[float] = None
    ) -> List[str]:
        """
        Fetch multiple URLs concurrently.
        
        Args:
            urls: URLs to fetch
            timeout: Request timeout
            
        Returns:
            List[str]: Response contents
        """
        tasks = [
            AsyncOperations.fetch_url(url, timeout)
            for url in urls
        ]
        return await asyncio.gather(*tasks)
    
    @staticmethod
    async def process_stream(
        stream_func: Callable,
        process_func: Callable,
        *args: Any
    ) -> None:
        """
        Process streaming data asynchronously.
        
        Args:
            stream_func: Function generating data
            process_func: Function processing data
            *args: Additional arguments
        """
        async for item in stream_func(*args):
            await process_func(item)
```

### Async Context Managers

```python
from typing import AsyncIterator, Optional
import aiofiles
import asyncio

class AsyncResource:
    """Async context manager example."""
    
    def __init__(self) -> None:
        self.name = "AsyncResource"
    
    async def __aenter__(self) -> 'AsyncResource':
        """Set up async resource."""
        print(f"Acquiring {self.name}")
        await asyncio.sleep(1)  # Simulate async setup
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """Clean up async resource."""
        print(f"Releasing {self.name}")
        await asyncio.sleep(1)  # Simulate async cleanup

class AsyncFileProcessor:
    """Process files asynchronously."""
    
    @staticmethod
    async def process_file(
        input_path: str,
        output_path: str,
        chunk_size: int = 8192
    ) -> None:
        """
        Process file asynchronously.
        
        Args:
            input_path: Input file path
            output_path: Output file path
            chunk_size: Processing chunk size
        """
        async with aiofiles.open(input_path, 'rb') as fin:
            async with aiofiles.open(output_path, 'wb') as fout:
                while chunk := await fin.read(chunk_size):
                    # Process chunk
                    processed = chunk.upper()
                    await fout.write(processed)
```

## Coroutines

### Generator-based Coroutines

```python
from typing import Generator, Any
from collections import deque

def consumer() -> Generator[None, Any, None]:
    """
    Consumer coroutine.
    
    Yields:
        None
        
    Returns:
        None
    """
    while True:
        item = yield
        print(f"Consuming {item}")

def producer(consumer: Generator) -> None:
    """
    Producer function.
    
    Args:
        consumer: Consumer coroutine
    """
    consumer.send(None)  # Prime the consumer
    for i in range(5):
        consumer.send(i)
```

### Async Generators

```python
from typing import AsyncGenerator, Any
import asyncio

class AsyncDataProcessor:
    """Process data asynchronously."""
    
    @staticmethod
    async def generate_data() -> AsyncGenerator[int, None]:
        """
        Generate data asynchronously.
        
        Yields:
            int: Generated data
        """
        for i in range(5):
            await asyncio.sleep(1)  # Simulate async work
            yield i
    
    @staticmethod
    async def process_data(
        generator: AsyncGenerator[Any, None]
    ) -> None:
        """
        Process data from async generator.
        
        Args:
            generator: Async generator
        """
        async for item in generator:
            # Process item
            result = item * 2
            print(f"Processed: {result}")
```

## Event Loops

### Custom Event Loop

```python
import asyncio
from typing import Optional, Callable, Any
import time

class CustomEventLoop:
    """Custom event loop implementation."""
    
    def __init__(self) -> None:
        self.loop = asyncio.new_event_loop()
        self._running = False
    
    def start(self) -> None:
        """Start the event loop."""
        asyncio.set_event_loop(self.loop)
        self._running = True
        self.loop.run_forever()
    
    def stop(self) -> None:
        """Stop the event loop."""
        self._running = False
        self.loop.stop()
        self.loop.close()
    
    def add_task(self, coro: Callable) -> None:
        """
        Add task to event loop.
        
        Args:
            coro: Coroutine to execute
        """
        self.loop.create_task(coro)
    
    def run_until_complete(self, coro: Callable) -> Any:
        """
        Run until coroutine completes.
        
        Args:
            coro: Coroutine to execute
            
        Returns:
            Any: Coroutine result
        """
        return self.loop.run_until_complete(coro)
```

## Concurrent Execution

### Task Groups

```python
import asyncio
from typing import List, Any, Callable
import time

class TaskGroup:
    """Manage groups of tasks."""
    
    def __init__(self) -> None:
        self.tasks: List[asyncio.Task] = []
    
    async def spawn(self, coro: Callable) -> None:
        """
        Spawn new task.
        
        Args:
            coro: Coroutine to execute
        """
        task = asyncio.create_task(coro)
        self.tasks.append(task)
    
    async def wait_all(self) -> List[Any]:
        """
        Wait for all tasks to complete.
        
        Returns:
            List[Any]: Task results
        """
        if not self.tasks:
            return []
        
        done, pending = await asyncio.wait(self.tasks)
        results = []
        
        for task in done:
            try:
                result = await task
                results.append(result)
            except Exception as e:
                print(f"Task failed: {e}")
        
        self.tasks.clear()
        return results
```

### Rate Limiting

```python
import asyncio
from typing import Callable, Any
import time

class RateLimiter:
    """Rate limit async operations."""
    
    def __init__(
        self,
        max_rate: int,
        time_period: float = 1.0
    ) -> None:
        self.max_rate = max_rate
        self.time_period = time_period
        self.tokens = max_rate
        self.last_update = time.monotonic()
        self.lock = asyncio.Lock()
    
    async def acquire(self) -> None:
        """Acquire rate limit token."""
        async with self.lock:
            now = time.monotonic()
            time_passed = now - self.last_update
            
            # Replenish tokens
            self.tokens = min(
                self.max_rate,
                self.tokens + time_passed * (self.max_rate / self.time_period)
            )
            
            if self.tokens < 1:
                wait_time = (1 - self.tokens) * (self.time_period / self.max_rate)
                await asyncio.sleep(wait_time)
                self.tokens = 1
            
            self.tokens -= 1
            self.last_update = now
    
    async def run(self, func: Callable, *args: Any) -> Any:
        """
        Run function with rate limiting.
        
        Args:
            func: Function to execute
            *args: Function arguments
            
        Returns:
            Any: Function result
        """
        await self.acquire()
        return await func(*args)
```

## Synchronization

### Async Locks

```python
import asyncio
from typing import Optional
import time

class AsyncLock:
    """Custom async lock implementation."""
    
    def __init__(self) -> None:
        self._lock = asyncio.Lock()
        self._owner: Optional[int] = None
    
    async def acquire(self) -> bool:
        """
        Acquire the lock.
        
        Returns:
            bool: True if lock acquired
        """
        result = await self._lock.acquire()
        if result:
            self._owner = id(asyncio.current_task())
        return result
    
    async def release(self) -> None:
        """Release the lock."""
        if self._owner != id(asyncio.current_task()):
            raise RuntimeError("Lock can only be released by owner")
        self._owner = None
        self._lock.release()
    
    async def __aenter__(self) -> 'AsyncLock':
        """Acquire lock using async context manager."""
        await self.acquire()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        """Release lock using async context manager."""
        await self.release()
```

### Semaphores

```python
import asyncio
from typing import Optional, Set
import time

class BoundedSemaphore:
    """Custom bounded semaphore implementation."""
    
    def __init__(self, value: int = 1) -> None:
        self._value = value
        self._initial_value = value
        self._lock = asyncio.Lock()
        self._waiters: Set[asyncio.Future] = set()
    
    async def acquire(self) -> bool:
        """
        Acquire semaphore.
        
        Returns:
            bool: True if acquired
        """
        async with self._lock:
            while self._value <= 0:
                waiter = asyncio.get_running_loop().create_future()
                self._waiters.add(waiter)
                try:
                    await waiter
                finally:
                    self._waiters.remove(waiter)
            self._value -= 1
            return True
    
    async def release(self) -> None:
        """Release semaphore."""
        async with self._lock:
            if self._value >= self._initial_value:
                raise ValueError("Semaphore released too many times")
            
            self._value += 1
            if self._waiters:
                waiter = next(iter(self._waiters))
                waiter.set_result(None)
```

## Best Practices

### Error Handling

```python
import asyncio
from typing import Optional, Callable
import functools
import time

def async_retry(
    retries: int = 3,
    delay: float = 1.0,
    backoff: float = 2.0
) -> Callable:
    """
    Retry decorator for async functions.
    
    Args:
        retries: Number of retries
        delay: Initial delay
        backoff: Delay multiplier
        
    Returns:
        Callable: Decorated function
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args: Any, **kwargs: Any) -> Any:
            current_delay = delay
            
            for attempt in range(retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    if attempt == retries - 1:
                        raise
                    
                    print(
                        f"Attempt {attempt + 1} failed: {e}. "
                        f"Retrying in {current_delay}s..."
                    )
                    await asyncio.sleep(current_delay)
                    current_delay *= backoff
        
        return wrapper
    return decorator
```

### Resource Management

```python
from typing import AsyncIterator, Optional
import asyncio
import time

class ResourcePool:
    """Manage pool of async resources."""
    
    def __init__(
        self,
        factory: Callable[[], Any],
        max_size: int = 10
    ) -> None:
        self.factory = factory
        self.max_size = max_size
        self.size = 0
        self.available: asyncio.Queue = asyncio.Queue()
        self.lock = asyncio.Lock()
    
    async def acquire(self) -> Any:
        """
        Acquire resource from pool.
        
        Returns:
            Any: Acquired resource
        """
        async with self.lock:
            if not self.available.empty():
                return await self.available.get()
            
            if self.size < self.max_size:
                self.size += 1
                return await self.factory()
            
            return await self.available.get()
    
    async def release(self, resource: Any) -> None:
        """
        Release resource back to pool.
        
        Args:
            resource: Resource to release
        """
        await self.available.put(resource)
```

This enhanced version of Concurrency and Asynchronous Programming provides comprehensive coverage of Python's concurrency features, with practical examples and modern best practices. The content is organized for both learning and reference, with a focus on type hints, error handling, and efficient resource management.
